# -*- coding: utf-8 -*-
"""Esercitazione ML ALV_18_04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xs1-HA4AOzkPjEK2z7XQY8_kp-V_3YO1

# Linear Regression in Practice

This notebook is organized as a sequence of small exercises that you should work **in this notebook**. Some important tips:

 1. Make sure you **document** your solutions, don't just write code -- even if it works perfectly. Use the rich markup abilities of Jupyter to explain each of your solutions and analyze results.

 1. The exercises are all related, so it is a good idea to read ahead to see, for example, the features you might want your **pipeline** to have before implementing it.

 2. Don't change the structure of the notebook. Keep the overall organization as in the original.

 3. Feel free to export the notebook to a Jupyter .ipnb notebook and work on your own Jupyter installation.

 4. For the **final submission** you should submit an .ipnb file, so if you use Google Colaboratory you will need to **export** it to upload your submission.


**VERY IMPORTANT**: Make sure you "Save a copy in Drive" from the File menu so that you can save your changes.

In this lab we will work through an extended example of exploratory data analysis and supervised machine learning using the California Housing Price Dataset. This dataset consists of data about housing characteristics and prices in many districts of the state of California. The **task** this dataset asks us to solve is estimating the median house value in a district from a set of independent housing characteristics.

**Note**: the exercises are inline in this notebook and *not* at the end. The exercises will ask you to write some code and sometimes to provide some analysis of your findings in Markdown cells at the end of the exercise.

## Part 1: Warming Up

In this first set of exercises we will analyze our dataset and build a simple linear regression pipeline. This is a fairly typical task that is asked of anyone working with Data Science: Here is some data, do something useful with it!

### Step 1: Data Modeling

OK, let's get started. The first thing we want to do is get our dataset loaded and start to get a feel for it. This is always a good idea -- we *play* with the data first in order to get a better understanding of it.
"""

# Initial imports -- these are fairly standard.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Import the function that will download the dataset.
from sklearn.datasets import fetch_california_housing

# Load the sklearn version of the California Housing dataset.
ds = fetch_california_housing()

"""### Exercise 1a: Poking Around

Spend some time looking at the elements of the `ds` we just loaded (it's a python `dict`). Find the description of the dataset and make sure you understand what the features are and what the targets variable is. **Hints**: to get the keys of the dictionary, use: `ds.keys()`.

We are going to construct a Pandas `DataFrame` in the next exercise. Where can you get reasonable column names from the sklearn dataset object?
"""

# Your code here.

# We print the keys

print('keys')
print(ds.keys())
print()

# We print Data, Target..
print('data')
print(ds['data'])
print()
print('target')
print(ds['target'])
print()
print('frame')
print(ds['frame'])
print()
print('target_names')
print(ds['target_names'])
print()
print('feature_names')
print(ds['feature_names'])
print()
print('DESCR')
print(ds['DESCR'])

"""**Your Analysis**: Modify this Markdown cell with the results of your playing around in the above code cell. The point of Notebooks is to build self-documenting, executable, and reproducible analyses of your work -- so **document** on the way.

The keys contained in the the California Housing data set are:

*  Data: it shows an array of 20640 rows and 8 columns corresponding to the 8 different features.
*  Target: it contains an array of 20640 rows with the target variable:
MedHouseVal - the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).
*  Frame: it contains nothing
*  Target_names: it contains the target variable name: MedHouseVal
*  Feature_names: it contains the features names
*  DESCR: It contains information about the data set, the different features and the references.

Here is the 8 features of the Data Set:

        - MedInc        median income in block group
        - HouseAge      median house age in block group
        - AveRooms      average number of rooms per household
        - AveBedrms     average number of bedrooms per household
        - Population    block group population
        - AveOccup      average number of household members
        - Latitude      block group latitude
        - Longitude     block group longitude

Here is two more informations contains in DESCR relevant for the analysis:

* This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

* A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

### Exercise 1b: Creating a Pandas DataFrame

OK, now we can create the `DataFrame` to hold our independent variables and a `Series` to hold the target values. Make sure you use good column names when constructing the `DataFrame`. Some relevant documentation: [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and [pandas.Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html).
"""

# Create a Pandas DataFrame for our dataset and a Pandas Series for the targets.

# Your code to build the DataFrame here (replace None)
index=np.arange(1,20640,1)
print(index)
df = pd.DataFrame(data=ds['data'], index=np.arange(1,20641,1), columns=ds['feature_names'], dtype=np.float64, copy=True)
print('Dataframe Info')
df.info()
print()
# Your code to build the target Series here (replace None)
targets = pd.Series(data=ds['target'], index=np.arange(1,20641,1), dtype=np.float64, name=ds['target_names'][0], copy=True)
print('Targets Info')
targets.info()

"""### Exercise 1c: Examining the Data

Study the *descriptive statistics* of the data. Do you notice anything "strange" about any of the features? Are the features scaled similarly? **Hint**: Use the `.describe()` method on the DataFrame you created.
"""

# Your code here.

print('Descriptive statistics of the features')
print(df.describe())
print()
print('Descriptive statistics of the Target')
print(targets.describe())

"""**Your Analysis Here** (in Markdown).

> The mean value for our target variable is 2.068 -> 206.800 $ of MedHouse Value.

> The std dev of the target value is high 1.15 -> 115.000$

> All feature variables have a very wide range except for the geographical feautures (Latitude, Longitude) and House Age. Searching Online it seems that the HouseAge value is capped to 52 years for 52+ years old houses.

> Features do not have the same units of measurement($, years, Average # rooms/bedroom, # population, coordinates).

> std dev seems to be high for HouseAge and AveOccup.

> More or less the median income is normally distributed but there is some people getting a high salary.


> It seems that we have a cap for high-valued houses: all houses with a price above 5 are given the value 5.

---
### Step 2: Visualization

OK, now that we have a bit of a *feel* for our data, let's get a better idea about it through visualization.

### Exercise 2a: Visualizing the Target
Create a plot to study the **distribution** of our target values. The best tool for that is a **histogram**. Search for this functionality in the Matplotlib documentation.

**Note**: In addition to *histograms*, try out the Seaborn function `distplot`.
"""

# Your visualization code here.

#MatplotLib

mu = targets.mean()
sigma = targets.std()
print(mu)
x = targets

num_bins = 100

n, bins, patches = plt.hist(x, num_bins,
                            density = 1,
                            color ='green',
                            alpha = 0.7)

y_normale = ((1 / (np.sqrt(2 * np.pi) * sigma)) *
     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))
plt.plot(bins, y_normale, '--', color ='red')

plt.axvline(x = mu,  color = 'b', label = 'mean')



plt.xlabel('MedHouseVal')
plt.ylabel('frequency')

plt.title('matplotlib.pyplot.hist()',
          fontweight ="bold")

plt.show()

#Distplot
import seaborn as sea
sea.histplot(data=targets, bins=num_bins,stat='count')

plt.xlabel('MedHouseVal')
plt.ylabel('count')

plt.title('sea.histplot()',
          fontweight ="bold")

"""**Your Analysis Here**.

> From the histogram, plotted first with the MatplotLib library and then with the Seaborn library, we can see how considerably the distribution of the target variable differs from a normal one with the same mean and variance; in fact, the empirical distribution seems to show a positive skewness, with below-average values much more frequent than above-average values.

> Important note: there is the presence of an outlier frequency: we have a particularly high frequency on the $500.000 value, compared to nearby volumes; probably the median house value for certain districts that were worth 500k+  were capped at 500.000 in the database.

### Exercise 2b: Subplots
Now create a multi-plot figure to visualize the distributions of **all** of the independent features in the dataset. Make sure you use `figsize=` to resize the figure appropriately.

A few things that will help with this:
+ If you want to index columns by **integer** indices, use the `.iloc()` method (e.g. `df.iloc[:,1]`).
+ If you extract a column as a `Series` from a `DataFrame`, you can recover its name with the `name` attribute.
+ Encapsulate you plotting code in a **function** you can call later.

**Super Hint**: Pandas already has this functionality **built-in**. If you can find it, use it!
"""

def plot_hist(data, n, bins, statistic,n_rows,n_cols):
 fig=plt.figure(figsize=(n_cols*6,n_rows*3))
 for i in range(n):
   ax=fig.add_subplot(n_rows,n_cols,i+1)
   feature = data.iloc[:,i]
   feature_name = data.iloc[:,i].name
   sea.histplot(data=feature, bins=bins, ax=ax, stat=statistic)
   plt.xlabel(feature_name)
   plt.ylabel(statistic)
   plt.title(feature_name, fontweight ="bold")
 fig.tight_layout()  # Improves appearance a bit.
 plt.show()
 return

# Your code here.

num_bins = 100

plot_hist(data=df, n = 8, bins=num_bins, statistic='count',n_rows = 4 , n_cols= 2)

def plot_scatter(data, n, y,  n_rows,n_cols):
 fig=plt.figure(figsize=(n_cols*6,n_rows*3))
 for i in range(n):
   ax=fig.add_subplot(n_rows,n_cols,i+1)
   feature = data.iloc[:,i]
   feature_name = data.iloc[:,i].name
   sea.scatterplot(x = feature,
                  y = y,
                  data = data,
                  #size = targets,
                  hue = targets,
                  palette = 'Spectral_r')
   plt.xlabel(feature_name)
   plt.ylabel('scatter')
   plt.title(feature_name, fontweight ="bold")
 fig.tight_layout()  # Improves appearance a bit.
 plt.show()
 return

"""---
## Step 3: Split your Data

A very important step. Now we will split our `DataFrame` into training and testing splits.

### Exercise 3.1: Create a Split
Now we need to create our training and testing splits. Read the documentation for `sklearn.model_selection.train_test_split()`. Use this function to create a **training** split with 75% of the data, and a **test** split with 25% of the data.
"""

# Your code here.
from sklearn.model_selection import train_test_split



# Split data into 75-25 train/test split -- replace the [None]*4 with your code.

test_size=1/4

(Xtr, Xte, ytr, yte) = train_test_split(df, targets, test_size=test_size, random_state=4)
print('Splitting Info')
print('--------------------------------------------------')
print('Training Set size             Test Set Size')
print('---------------------------------------------------')
print(f'{str(Xtr.shape):>10}{"                       "}{str(Xte.shape):>8}')

#Scatterplots Features vs Target (Training Set)
print('--------------------------------------------------------------------------------')
print("                          " + "Scatterplot Training Set" + "                    ")
print('--------------------------------------------------------------------------------')
plot_scatter(data=Xtr, n = 8, y = ytr, n_rows = 4 , n_cols= 2)

#Scatterplots Features vs Target (Test Set)

print('--------------------------------------------------------------------------------')
print("                          " + "Scatterplot Test Set" + "                    ")
print('--------------------------------------------------------------------------------')

plot_scatter(data=Xte, n = 8, y = yte, n_rows = 4 , n_cols= 2)

"""### Exercise 3.2: Fit a LinearRegression
Finally some machine learning. Study the documentation for `class sklearn.linear_model.LinearRegression`. Then write some code to fit a linear regression model to your **training** split. Try out your model by computing predictions on some data (use the `model.predict()` method).

"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

#It can be useful to standardize the futures, since they are xpressed in different units of measure

scaler = StandardScaler()
scaler.fit(Xtr)

Xtr_s = pd.DataFrame(scaler.transform(Xtr), columns=ds['feature_names'])
Xte_s = pd.DataFrame(scaler.transform(Xte), columns=ds['feature_names'])

# Your code here.
model = LinearRegression()
fit_tr = model.fit(Xtr_s, ytr)

print('R^2')
r_2 = model.score(Xtr_s, ytr)
print(r_2)

print()
print('Coefficients estimated in the Training set')

print()
print('Intercept Value: ')
print(f'-> {model.intercept_}')

print()
for i in range(8):
   feature_name = df.iloc[:,i].name
   print(feature_name + ': ')
   print(f'-> {fit_tr.coef_[i]}')

predict_tr = model.predict(Xtr_s)

print()

sea.scatterplot(x = ytr,
                y = predict_tr,
                #size = targets,
                hue = ytr,
                palette = 'coolwarm')
plt.plot(ytr,
         ytr)
plt.xlabel('Target Values in training Set')
plt.ylabel('predict_tr')
plt.title('MedHouseVal Predicted vs Training Set', fontweight = "bold")

plt.show()

"""### Exercise 3.3: Evaluate your Model
Write some code to compute the root mean-squared error (RMSE) and mean absolute error (MAS) for you model predictions. Try it on both the **test** and **training** splits.
"""

# Your code here.

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

predict_te = model.predict(Xte_s)

print('--------------------------------------------------------------------------------')
print("                          " + "RMSE/MAE Training Set (75%)" + "                    ")
print('--------------------------------------------------------------------------------')

RMSE_tr = np.sqrt(mean_squared_error(predict_tr, ytr))
MAE_tr = mean_squared_error(predict_tr, ytr)

print("RMSE", RMSE_tr)
print("MAE",MAE_tr)

print('--------------------------------------------------------------------------------')
print("                          " + "RMSE/MAE Test Set (25%)" + "                    ")
print('--------------------------------------------------------------------------------')

RMSE_te = np.sqrt(mean_squared_error(predict_te, yte))
MAE_te = mean_squared_error(predict_te, yte)

print("RMSE", RMSE_te)
print("MAE",MAE_te)

print()

sea.scatterplot(x = yte,
                y = predict_te,
                #size = targets,
                hue = yte,
                palette = 'coolwarm')
plt.plot(yte,
         yte)
plt.xlabel('Target Values in Test Set')
plt.ylabel('predict_te')
plt.title('MedHouseVal Predicted vs Test Set', fontweight = "bold")

plt.show()

# Evaluation of RMSE/MAE for different proportions of the data set


for j in range(9):
 j = j+1
 i = np.arange(0.1,1,0.1)
 test_size= i[j-1]
 (Xtr1, Xte1, ytr1, yte1) = train_test_split(df, targets, test_size=test_size, random_state=4)

 model1 = LinearRegression()
 fit_tr1 = model1.fit(Xtr1, ytr1)
 predict_tr1 = model1.predict(Xtr1)
 predict_te1 = model1.predict(Xte1)

 print('--------------------------------------------------------------------------------')
 print("                          " + "MSE/MAE Training Set (" + str("{:.1f}".format(1-test_size)) + ")                    ")
 print('--------------------------------------------------------------------------------')

 RMSE_tr1 = np.sqrt(mean_squared_error(predict_tr1, ytr1))
 MAE_tr1 = mean_squared_error(predict_tr1, ytr1)

 print("RMSE", RMSE_tr1)
 print("MAE",MAE_tr1)

 print('--------------------------------------------------------------------------------')
 print("                          " + "MSE/MAE Test Set (" + str("{:.1f}".format(test_size)) +")                    ")
 print('--------------------------------------------------------------------------------')

 RMSE_te1 = np.sqrt(mean_squared_error(predict_te1, yte1))
 MAE_te1 = mean_squared_error(predict_te1, yte1)

 print("RMSE", RMSE_te1)
 print("MAE",MAE_te1)
 print()
 print('R^2')
 r_2 = model.score(Xtr1, ytr1)
 print(r_2)
 print()

 plt.plot(test_size,
                RMSE_tr1,
                marker="o", markersize=6, markeredgecolor="red", markerfacecolor="green")
 plt.plot(test_size,
                MAE_tr1,
                marker="o", markersize=6, markeredgecolor="blue", markerfacecolor="green")
 plt.plot(test_size,
                RMSE_te1,
                marker="o", markersize=6, markeredgecolor="red", markerfacecolor="yellow")
 plt.plot(test_size,
                MAE_te1,
                marker="o", markersize=6, markeredgecolor="blue", markerfacecolor="yellow")

legend = plt.legend(['RMSE_tr','MAE_tr','RMSE_te','MAE_te'], title = "Legend")
legend.set_title("Legend", prop = {'size':15})
plt.xlabel('Test Set Size')
plt.ylabel('RMSE/MSE')
plt.title('MedHouseVal RMSE and MAE', fontweight = "bold")

plt.show()

"""**Your Analysis Here**: Why is the performance on the train set different than that on the test split? What if you change the proportion of training to test data in your splits?

The performance changes since the data in which we test our model are different from our training set. The errors seem to be a little higher on our test set, but there is not much difference.

The errors tends to rise when we take a training size smaller than 50%, while for training set sizes higher than 50%, it seems that the errors don't change much; as an extreme example, when we take a very small training set size -> 1/10 proportion (10% training set, 90% test set), our errors spike (the training set is too small to produce a model which generates good estimates in the test set).

### Exercise 3.4: Visualizing the Results
Now I want you to write a function that makes a **residual plot** of the data and the model predictions. This plot should show, for each data point, the **signed error** (i.e. y - predicted) of the model prediction. Do you notice any **patterns** in the errors? Can you link this to previous analyses you made?
"""

# Your code here.

sea.scatterplot(x = yte,
                y = yte - predict_te,
                #size = targets,
                hue = yte,
                palette = 'coolwarm')
zeros = np.zeros((len(yte),))

list1 = yte - predict_te
pos_count, neg_count = 0, 0

# iterating each number in list
for num in list1:

    # checking condition
    if num >= 0:
        pos_count += 1

    else:
        neg_count += 1

print("Y_te > Predict (underestimates):", pos_count)
print("Y_te < Predict (overestimates):", neg_count)

plt.plot(yte,
         zeros)
plt.xlabel('Y -> MedHousVal in Test Set (Mln $)')
plt.ylabel('Y - Y_predicted')
plt.title('MedHouseVal Errors vs Y (Test Set)', fontweight = "bold")


plt.show()

sea.scatterplot(x = predict_te,
                y = yte - predict_te,
                #size = targets,
                hue = yte,
                palette = 'coolwarm')
zeros = np.zeros((len(yte),))
plt.plot(yte,
         zeros)
plt.xlabel('Y_predicted -> MedHousVal in Test Set (Mln $)')
plt.ylabel('Y - Y_predicted')
plt.title('MedHouseVal Residuals (Test Set vs Predicted)', fontweight = "bold")


plt.show()

"""**Your Analysis Here**

We can clearly see how our model tends to overestimate a little bit the MedHouseVal for lower values of the Target variable and tends to underestimate by a large amount the MedHouseVal for higher values of the Target variable. The errors seem to have a positive, slope different from zero. It seems that there is a distortion in our estimates. Finally, by counting positive and negative residuals), we have the tendency to overestimate our predictions more than over underestimate them (probably because we have more observations for lower values of the target and in which we observe more overestimation).

Our dataset is full of outliers and thresholds (ex. max value for House age, for our target variable..), so maybe these distortions influence the bias we observe in our estimates.

### Step 4: Repeat.

Now you should put all of the pieces together into a repeatable, reproducible pipeline.

### Exercise 4: The Pipeline
Write a function (or even just code in the cell that calls previously defined functions) that runs an **experiment**:
1. Splitting data
1. Instantiating the model
1. Fitting the model
1. Evaluating the model
1. (Maybe) Visualizing results

Experiment with different splits to see if the results are the same. Try using more or less training data with respect to test data. Observe how the results change.
"""

# Your pipeline code here.
def pipeline(model, df, targets, train_size,Random_State):

  #Split data
  (Xtr, Xte, ytr, yte) = train_test_split(df, targets, test_size = 1 - train_size, random_state= Random_State)
  #print Split
  print('Splitting Info')
  print('--------------------------------------------------')
  print('Training Set size             Test Set Size')
  print('---------------------------------------------------')
  print(f'{str(Xtr.shape):>10}{"                       "}{str(Xte.shape):>8}')

  print()
  #Standardize features

  scaler = StandardScaler()
  scaler.fit(Xtr)
  Xtr_s = pd.DataFrame(scaler.transform(Xtr))
  Xte_s = pd.DataFrame(scaler.transform(Xte))
  #Estimate Model
  fit_tr = model.fit(Xtr_s, ytr)

  #Print R2 and Coefficient Estimates
  print('R^2 Training Set')
  r_2 = model.score(Xtr_s, ytr)
  print(r_2)

  print()
  print('Coefficients estimated in the Training set')

  print()
  print('Intercept Value: ')
  print(f'-> {model.intercept_}')

  print()

  for i in range(8):
   feature_name = df.iloc[:,i].name
   print(feature_name + ': ')
   print(f'-> {fit_tr.coef_[i]}')

  predict_tr = model.predict(Xtr_s)

  print()

  #Evaluate the model on the Test Set
  predict_te = model.predict(Xte_s)

 #Compute and print RMSE and MAE
  print('--------------------------------------------------------------------------------')
  print("                          " + "RMSE/MAE Training Set ("  + str("{:.2f}".format(train_size)) +")                    ")
  print('--------------------------------------------------------------------------------')

  RMSE_tr = np.sqrt(mean_squared_error(predict_tr, ytr))
  MAE_tr = mean_squared_error(predict_tr, ytr)

  print("RMSE", RMSE_tr)
  print("MAE",MAE_tr)

  print('--------------------------------------------------------------------------------')
  print("                          " + "MSE/MAE Test Set (" + str("{:.2f}".format(1 - train_size)) +")                    ")
  print('--------------------------------------------------------------------------------')

  RMSE_te = np.sqrt(mean_squared_error(predict_te, yte))
  MAE_te = mean_squared_error(predict_te, yte)

  print("RMSE", RMSE_te)
  print("MAE",MAE_te)

  print()

  #scatterplot of the residuals

  sea.scatterplot(x = predict_te,
                y = yte - predict_te,
                #size = targets,
                hue = yte,
                palette = 'coolwarm')
  zeros = np.zeros((len(yte),))
  plt.plot(yte,
         zeros)
  plt.xlabel('Y_predicted -> MedHousVal in Test Set (Mln $)')
  plt.ylabel('Y - Y_predicted')
  plt.title('MedHouseVal Residuals (Test Set vs Predicted)', fontweight = "bold")


  plt.show()
  print()

#Print Pipeline Results for 10 diffetent splits
for j in range(9):
 train_size = np.arange(0.1,1,0.1)
 train_size = train_size[j]
 pipeline(model = LinearRegression(), df = df, targets = targets, train_size =train_size, Random_State = 4)

"""**Your Analysis Here**: Experiment with different splits to see if the results are the same. Try using more or less training data with respect to test data. Observe how the results change.

We built our pipeline by scaling our variables, fitting, predicting and then printing the residuals.

As we have seen previously, our simple linear regression seem to have a distortion in the estimates (overestimation for lower values of the target and underestimation for higher values of the target variable).

The first Training/test split size proportion seem to be reasonable (25/75 -> Test/Training), since the errors tends to rise when we take a training size smaller than 50%, while for training set sizes higher than 50%, it seems that the errors don't change much.

---
---
## Part 2: Improving our Regressor

Now that we have a simple, baseline linear regression result, let's see if we can't improve on it. This is where the real work begins, and where it is **super** important to ensure that the conclusions we draw are *valid*.

**Questions**: Are our independent variables *scaled* similarly? Does our model have *high variance* -- that is, if we fit it to a new training sample, does the result vary dramatically?

### Exercise 5: Increasing Model Capacity

Check out the documentation for `sklearn.preprocessing.PolynomialFeatures`. Map the independent variables onto a **polynomial** basis of variable order. Fit your model using your pipeline from above and observe its behavior for different degree polynomial embeddings.
"""

from sklearn.preprocessing import PolynomialFeatures

# We define a little different pipeline without printing of coefficients
def pol_pipeline(degree, model, df, targets, train_size, Random_State):

  #print Degree
  print('Degree')
  print('--------------------------------------------------')
  print(degree)
  print('---------------------------------------------------')

  Polynomial = PolynomialFeatures(degree = degree)
  X_polynomial = Polynomial.fit_transform(df)
  #Split data
  (Xtr, Xte, ytr, yte) = train_test_split(X_polynomial, targets, test_size = 1 - train_size, random_state= Random_State)
  #print Split
  print('Splitting Info')
  print('--------------------------------------------------')
  print('Training Set size             Test Set Size')
  print('---------------------------------------------------')
  print(f'{str(Xtr.shape):>10}{"                       "}{str(Xte.shape):>8}')

  print()
  # Standardize features

  scaler = StandardScaler()
  scaler.fit(Xtr)
  Xtr_s = pd.DataFrame(scaler.transform(Xtr))
  Xte_s = pd.DataFrame(scaler.transform(Xte))

  #Estimate Model

  fit_tr = model.fit(Xtr_s, ytr)
  predict_tr = model.predict(Xtr_s)

  #Print R2
  print('R^2 Training Set')
  r_2 = model.score(Xtr_s, ytr)
  print(r_2)
  print()

 #Print R2
  print('R^2 Test Set')
  r_2 = model.score(Xte_s, yte)
  print(r_2)
  print()

  #Evaluate the model on the Test Set
  predict_te = model.predict(Xte_s)

 #Compute and print RMSE and MAE
  print('--------------------------------------------------------------------------------')
  print("                          " + "RMSE/MAE Training Set ("  + str("{:.2f}".format(train_size)) +")                    ")
  print('--------------------------------------------------------------------------------')

  RMSE_tr = np.sqrt(mean_squared_error(predict_tr, ytr))
  MAE_tr = mean_squared_error(predict_tr, ytr)

  print("RMSE", RMSE_tr)
  print("MAE",MAE_tr)

  print('--------------------------------------------------------------------------------')
  print("                          " + "MSE/MAE Test Set (" + str("{:.2f}".format(1 - train_size)) +")                    ")
  print('--------------------------------------------------------------------------------')

  RMSE_te = np.sqrt(mean_squared_error(predict_te, yte))
  MAE_te = mean_squared_error(predict_te, yte)

  print("RMSE", RMSE_te)
  print("MAE",MAE_te)
  print()

  #scatterplot of the residuals

  sea.scatterplot(x = predict_te,
                y = yte - predict_te,
                #size = targets,
                hue = yte,
                palette = 'coolwarm')
  zeros = np.zeros((len(yte),))
  plt.plot(yte,
         zeros)
  plt.xlabel('Y_predicted -> MedHousVal in Test Set (Mln $)')
  plt.ylabel('Y - Y_predicted')
  plt.title('MedHouseVal Residuals (Test Set vs Predicted)', fontweight = "bold")

  plt.show()
  print()
  return RMSE_tr, MAE_tr, RMSE_te, MAE_te

# Your code here.


#Print Pipeline Results for 5 different degrees
RMSE_tr_l = []
MAE_tr_l =[]
RMSE_te_l =[]
MAE_te_l =[]

for j in range(5):
 j = j+1
 train_size = 0.75
 RMSE_tr, MAE_tr, RMSE_te, MAE_te = pol_pipeline(degree = j, model = LinearRegression(), df = df, targets = targets, train_size = train_size, Random_State = 4)
 RMSE_tr_l.append(RMSE_tr)
 MAE_tr_l.append(MAE_tr)
 RMSE_te_l.append(RMSE_te)
 MAE_te_l.append(MAE_te)

#Print RMSE and MAE Results for the first 3 degrees (degre 4 and 5 are too high)
for j in range(3):
 plt.plot(j+1,
                RMSE_tr_l[j],
                marker="o", markersize=6, markeredgecolor="red", markerfacecolor="green")
 plt.plot(j+1,
                MAE_tr_l[j],
                marker="o", markersize=6, markeredgecolor="blue", markerfacecolor="green")
 plt.plot(j+1,
                RMSE_te_l[j],
                marker="o", markersize=6, markeredgecolor="red", markerfacecolor="yellow")
 plt.plot(j+1,
                MAE_te_l[j],
                marker="o", markersize=6, markeredgecolor="blue", markerfacecolor="yellow")

legend = plt.legend(['RMSE_tr','MAE_tr','RMSE_te','MAE_te'], title = "Legend")
legend.set_title("Legend", prop = {'size':15})
plt.xlabel('Degree')
plt.ylabel('RMSE/MSE')
plt.title('MedHouseVal Errors Predicted vs Test Set for different degrees', fontweight = "bold")

plt.show()

"""### Exercise 6: Hyperparameter Selection and Cross-validation

How should we select the correct *degree* for our polynomial basis? Is the performance on the *training* set equal to the performance on the *test* set? Why?
"""

# We define a little different pipeline without print of coefficients
def pol_pipeline_noprint(degree, model, df, targets, train_size, Random_State):

  Polynomial = PolynomialFeatures(degree = degree)
  X_polynomial = Polynomial.fit_transform(df)
  #Split data
  (Xtr, Xte, ytr, yte) = train_test_split(X_polynomial, targets, test_size = 1 - train_size, random_state= Random_State)

  # Standardize features

  scaler = StandardScaler()
  scaler.fit(Xtr)
  Xtr_s = pd.DataFrame(scaler.transform(Xtr))
  Xte_s = pd.DataFrame(scaler.transform(Xte))

  #Estimate Model

  fit_tr = model.fit(Xtr_s, ytr)
  predict_tr = model.predict(Xtr_s)

  #Evaluate the model on the Test Set
  predict_te = model.predict(Xte_s)

  #R2 Test
  r_2_test = model.score(Xte_s, yte)

 #Compute and print RMSE and MAE

  RMSE_tr = np.sqrt(mean_squared_error(predict_tr, ytr))
  MAE_tr = mean_squared_error(predict_tr, ytr)

  RMSE_te = np.sqrt(mean_squared_error(predict_te, yte))
  MAE_te = mean_squared_error(predict_te, yte)

  return Xtr_s, ytr, RMSE_tr, MAE_tr, RMSE_te, MAE_te, r_2_test, yte, predict_te

from sklearn.model_selection import cross_val_score

def cross_validation_fun(max_degree,df):
    #Calculates scoring for each function
    scores_RMSE_mean = np.zeros(max_degree)
    scores_MAE_mean = np.zeros(max_degree)
    scores_R2_mean = np.zeros(max_degree)
    for j in range(max_degree):
        Xtr_s, ytr, RMSE_tr, MAE_tr, RMSE_te, MAE_te, r_2_test, yte, predict_te = pol_pipeline_noprint(degree = j, model = LinearRegression(), df = df, targets = targets, train_size = train_size, Random_State = 4)
        scores_RMSE = cross_val_score(LinearRegression(), Xtr_s, ytr,cv = 5, n_jobs =4, scoring = 'neg_mean_squared_error')
        scores_MAE = cross_val_score(LinearRegression(), Xtr_s, ytr,cv = 5, n_jobs =4, scoring = 'neg_mean_absolute_error' )
        scores_R2 = cross_val_score(LinearRegression(), Xtr_s, ytr,cv = 5, n_jobs =4, scoring = 'r2' )
        scores_RMSE_mean[j] = - scores_RMSE.mean()
        scores_MAE_mean[j] = - scores_MAE.mean()
        scores_R2_mean[j] = scores_R2.mean()

    best_degree_RMSE = np.argmin(scores_RMSE_mean)
    best_degree_MAE = np.argmin(scores_MAE_mean)
    best_degree_R2 = np.argmax(scores_R2_mean)

    print()
    print('Cross Validation Results')
    print()
    print("Best polynomial degree according to RMSE:", best_degree_RMSE)
    print("Score_RMSE", scores_RMSE_mean[best_degree_RMSE])
    print()
    print("Best polynomial degree according to MAE:", best_degree_MAE)
    print("Score_MAE", scores_MAE_mean[best_degree_MAE])
    print()
    print("Best polynomial degree according to R2:", best_degree_R2)
    print("Score_R2", scores_R2_mean[best_degree_R2])
    print()
    return scores_RMSE_mean[best_degree_RMSE], scores_MAE_mean[best_degree_MAE], scores_R2_mean[best_degree_R2]

df.drop(['AveBedrms'], axis = 1)
df.drop(['AveOccup'], axis = 1)

max_degree = 5
cross_validation_fun(max_degree, df)

#Estimation of the Errors in a different random Split (with optimum degree)
pol_pipeline(degree = 2, model = LinearRegression(), df = df, targets = targets, train_size = 0.75, Random_State = 100)

#Estimation of the Errors in a different random Split (with optimum degree)
pol_pipeline(degree = 1, model = LinearRegression(), df = df, targets = targets, train_size = 0.75, Random_State = 100)

### Reduced Model

#Print Correlations

df2 = df
df2['MedHouseVal'] = targets
print(df2.corr())

#Reduced model

df_ = pd.DataFrame(data=ds['data'], index=np.arange(1,20641,1), columns=ds['feature_names'], dtype=np.float64, copy=True)
df1 = df_.drop(columns=['AveRooms','AveBedrms', 'Population', 'AveOccup'])

#Reduced model - RMSE e MAE for different degrees of the polynomial basis

#Print Pipeline Results for 7 different degrees
RMSE_tr_l = []
MAE_tr_l =[]
RMSE_te_l =[]
MAE_te_l =[]

for j in range(7):
 j = j+1
 train_size = 0.75
 Xtr_s, ytr, RMSE_tr, MAE_tr, RMSE_te, MAE_te, r_2_test, yte, predict_te = pol_pipeline_noprint(degree = j, model = LinearRegression(), df = df1, targets = targets, train_size = train_size, Random_State = 4)
 RMSE_tr_l.append(RMSE_tr)
 MAE_tr_l.append(MAE_tr)
 RMSE_te_l.append(RMSE_te)
 MAE_te_l.append(MAE_te)

#Print RMSE and MAE Results for different degrees of the polynomial basis
for j in range(7):
 plt.plot(j+1,
                RMSE_tr_l[j],
                marker="o", markersize=6, markeredgecolor="red", markerfacecolor="green")
 plt.plot(j+1,
                MAE_tr_l[j],
                marker="o", markersize=6, markeredgecolor="blue", markerfacecolor="green")
 plt.plot(j+1,
                RMSE_te_l[j],
                marker="o", markersize=6, markeredgecolor="red", markerfacecolor="yellow")
 plt.plot(j+1,
                MAE_te_l[j],
                marker="o", markersize=6, markeredgecolor="blue", markerfacecolor="yellow")

legend = plt.legend(['RMSE_tr','MAE_tr','RMSE_te','MAE_te'], title = "Legend")
legend.set_title("Legend", prop = {'size':15})
plt.xlabel('Degree')
plt.ylabel('RMSE/MSE')
plt.title('MedHouseVal Errors (Reduced model) - Predicted vs Test Set for different degrees', fontweight = "bold")

plt.show()

#Reduced model Cross validation
max_degree = 7

cross_validation_fun(max_degree, df = df1)

Xtr_s, ytr, RMSE_tr, MAE_tr, RMSE_te, MAE_te, r_2_test, yte, predict_te = pol_pipeline_noprint(degree = 5, model = LinearRegression(), df = df1, targets = targets, train_size = train_size, Random_State = 4)

sea.scatterplot(x = yte,
                y = predict_te,
                hue = yte,
                palette = 'coolwarm', alpha = 0.5)
plt.plot(yte,
         yte)
plt.xlabel('Target Values in Test Set')
plt.ylabel('predict_te')
plt.title('MedHouseVal Predicted vs Test Set (reduced model)', fontweight = "bold")

plt.show()
print('Descriptive statistics of the features')
print(df1.describe())
print()

#Estimation of the Errors in a different random Split (with optimum degree)
pol_pipeline(degree = 5, model = LinearRegression(), df = df1, targets = targets, train_size = 0.75, Random_State = 100)

list1 = yte - predict_te
pos_count, neg_count = 0, 0

# iterating each number in list
for num in list1:

    # checking condition
    if num >= 0:
        pos_count += 1

    else:
        neg_count += 1

print("Y_te > Predict (underestimates):", pos_count)
print("Y_te < Predict (overestimates):", neg_count)

#Learning curve for degree 5 - reduced model

from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(LinearRegression(), Xtr_s, ytr, cv = 5, scoring='neg_mean_squared_error')

train_sizes=np.linspace(0.01, 1.0, 5)
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.subplots(1, figsize=(10,10))
plt.plot(train_sizes, train_mean, '--', color="#111111",  label="Training score")
plt.plot(train_sizes, test_mean, color="#111111", label="Cross-validation score")

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color="#DDDDDD")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color="#DDDDDD")

plt.title("Learning Curve")
plt.xlabel("Training Set Size"), plt.ylabel("RMSE Score"), plt.legend(loc="best")
plt.tight_layout()
plt.show()



"""**Your Final Analysis Here**: Summarize the conclusions you can make about the best hyperparameter settings for this dataset. How do you know your conclusions are supported by the data?

We built our new pipeline (which take as input also the degree of the polynomial basis) by scaling our variables, fitting, predicting, without printing any results.

We decided to scale our variables like in the Linear regression model and to use the proportion Training/ Test (75/25) since it seems a reasonable proportion as we have seen before.

From the analysis of RMSE and MAE for different degrees of the polynomial basis of step 5 is clear that our choice should be between degree 1 and 2 if we go for the model with all the features embedded.

We used **cross validation in the complete model** and we can see how we should prefer degree 2 if we choose as our scoring function the MAE, while degree 1 if we choose RMSE or R2.

**Probably degree 1 for the complete model is preferrable,** since some of the variable are not so relevant and, with a degree of 2, the risk of overfitting and predicting badly in a random test set is high.

Heuristically, by having a look to the correlations in the scatterplots in exercise 3, we can try to drop some of the futures which are not so correlated with our target variable ('AveBedrms', 'AveRooms', 'Population', 'AveOccup') in order to build a **simpler reduced model** which tries to catch only the relation between the price of the houses (MedHouseVal) and the location ('Longitude', 'Latitude'), income of the block ('MedInc') and the House age.

**Reduced model results:** From the analysis of RMSE and MAE for different degrees of the polynomial basis and the results obtained performing cross validation, **our errors and R2 improve**, in particular if we choose the **optimal degree of 5**; maybe the higher polynomial basis in this case is due to the nonlinear relation between the target (MedHouseVal) and the location.

We perform our predictions with our reduced model (polynomial basis of degree 5) in a random test set and the overall performance of the reduced model seem to be better then the model predicting with all the features (both if we consider  degree 1 or degree 2).

Our predictions in the test set seem to be more accurate for lower values of the Target variable. However, the bias (overestimation in the predictions for lower values of the target and underestimation for higher values of the target) persists; In particular, our prediction is still very bad for y = 5. This is probably due to the threshold on the target variable value and the specificity of some observations in the dataset.

Finally, from the learning curve, we can see how our choice of splitting (training/test -  75/25) seems to be reasonable also for our reduced model.



"""